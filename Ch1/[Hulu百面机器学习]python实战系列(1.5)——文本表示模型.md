随着内容进一步发展，我们不得不面对组合特征或高维特征的问题。

**<font size=4 color= Magenta>Q: 什么是组合特征？如何处理高维难度组合特征？(2⭐)</font>**

为了提高复杂关系的拟合能力，我们时常把两个特征变量的$m$个特征和$n$个特征合在一块组成一个由$mn$个特征组成的向量。这种方法当引入ID类特征的时候，问题就出现了。这时需要引入一个较小的数$k \space (k << m,n)$，来达成降维目的。熟悉推荐算法的同学一眼就能看出是等价于矩阵分解(包括lu分解、qr分解和奇异值分解，详见百面机器学习系列的$(4.x)$)，他们都有各自独特的算法决定$k$值，使得特征数下降到$(m + n)k$，甚至到只有$k$个特征(这时的$k$可能还只是个位数)。

然而，可能会引发参数/特征过多、过拟合的问题，那如何筛选特征呢？

**<font size=4 color= Magenta>Q: 怎样有效地找到组合特征？(2⭐)</font>**

从根本上看，得用决策树算法(简单说就是根据不同特征的分叉去构建因变量模型)。机器学习这块有很多种决策树算法，在后面会一一展示出来。

因为很多东西都得放到后面去讲，这次的代码实战块主要集中在高维特征处理上：

```Python
import numpy as np
import pandas as pd
```

我们先考虑找$mn$个组合特征(数量非常大)，并赋值$1_{mn \times mn}$：
```Python
# (默认为上个模块的m和n)
Z = np.zeros((m*n,3+m*n))

Z[:,0] = np.random.randint(0,2,m*n)
Z[:,1] = np.repeat(np.arange(m),n)
Z[:,2] = list(range(n))*m

Z[:,-m*n:] = np.eye(m*n)
```

再去看看随机生成的用户和物品特征，直接筛选出组合特征数$k$(这样的坏处是$k$值仍然比较高)：

```Python
m = 100
n = 20
Z = np.zeros((m*n,2))

Z[:,0] = np.random.randint(0,m,m*n)
Z[:,1] = np.random.randint(0,n,m*n)

# 这次就用上numpy的多维寻找unique值的函数：
Y = np.unique(Z,axis = 0)
k = len(Y)
```

组合特征更是要被筛选出来的。sklearn,tensorflow和pytorch都没有相似的高阶特征组合工具，相应的算法耗时和占用空间就不进行比较了。这一期对构建机器学习框架没有什么帮助，那就

**敬请期待下一期：[Hulu百面机器学习]python实战系列(1.5)文本表示模型**

最后，我们的公众号精彩不断，欢迎大家关注哦：





